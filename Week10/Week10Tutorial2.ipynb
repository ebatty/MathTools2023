{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOp9OBOD7nMYmPOkl843SEX"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9l0lyqLvkW7"
      },
      "source": [
        "# Tutorial 2\n",
        "\n",
        "**Machine Learning II, Model Selection**\n",
        "\n",
        "**[insert your name]**\n",
        "\n",
        "**Important reminders**: Before starting, click \"File -> Save a copy in Drive\". Produce a pdf for submission by \"File -> Print\" and then choose \"Save to PDF\".\n",
        "\n",
        "To complete this tutorial, you should have watched Video 10.1, 10.2, and 10.3\n",
        "\n",
        "**This tutorial is inspired by and uses text/code from NMA W1D4**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cv9HSBNPyLV9",
        "cellView": "form"
      },
      "source": [
        "# @markdown Imports\n",
        "\n",
        "# Imports\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import ipywidgets as widgets  # interactive display\n",
        "import math\n",
        "\n",
        "import matplotlib as mpl\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIdPVYl9TzmK",
        "cellView": "form"
      },
      "source": [
        "# @markdown Plotting functions\n",
        "import warnings\n",
        "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "import numpy\n",
        "from numpy.linalg import inv, eig\n",
        "from math import ceil\n",
        "from matplotlib import pyplot, ticker, get_backend, rc\n",
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from itertools import cycle\n",
        "\n",
        "\n",
        "%config InlineBackend.figure_format = 'retina'\n",
        "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
        "\n",
        "def plot_weights(models, sharey=True):\n",
        "  \"\"\"Draw a stem plot of weights for each model in models dict.\"\"\"\n",
        "  n = len(models)\n",
        "  f = plt.figure(figsize=(10, 2.5 * n))\n",
        "  axs = f.subplots(n, sharex=True, sharey=sharey)\n",
        "  axs = np.atleast_1d(axs)\n",
        "\n",
        "  for ax, (title, model) in zip(axs, models.items()):\n",
        "\n",
        "    ax.margins(x=.02)\n",
        "    stem = ax.stem(model.coef_.squeeze(), use_line_collection=True)\n",
        "    stem[0].set_marker(\".\")\n",
        "    stem[0].set_color(\".2\")\n",
        "    stem[1].set_linewidths(.5)\n",
        "    stem[1].set_color(\".2\")\n",
        "    stem[2].set_visible(False)\n",
        "    ax.axhline(0, color=\"C3\", lw=3)\n",
        "    ax.set(ylabel=\"Weight\", title=title)\n",
        "  ax.set(xlabel=\"Neuron (a.k.a. feature)\")\n",
        "  f.tight_layout()\n",
        "\n",
        "def plot_model_quality(run_train, preds_train, run_test, preds_test, r2_train, r2_test):\n",
        "  fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
        "\n",
        "  axes[0].plot(run_train[:500],'k', label = 'True running speed')\n",
        "  axes[0].plot(preds_train[:500],'g',alpha=.7, label = 'Predicted running speed')\n",
        "\n",
        "  axes[0].legend()\n",
        "\n",
        "  axes[1].plot(run_test[:500],'k')\n",
        "  axes[1].plot(preds_test[:500],'g',alpha=.7)\n",
        "\n",
        "  axes[2].bar([0, 1], [r2_train, r2_test])\n",
        "\n",
        "  axes[0].set(title='Training data', xlabel='Time (bins)', ylabel='Running speed')\n",
        "  axes[1].set(title='Validation  data', xlabel='Time (bins)', ylabel='Running speed')\n",
        "  axes[2].set(title='Prediction Quality', ylabel='R2', xticks = [0, 1], xticklabels = ['Training data', 'Validation data']);\n",
        "\n",
        "\n",
        "def plot_regularized_train_and_val(alphas, reg_train_r2s, reg_validation_r2s, train_r2, validation_r2, regression_type):\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
        "\n",
        "    axes[0].plot([0, len(alphas)], [train_r2, train_r2], 'g', label= 'No reg')\n",
        "    axes[0].plot(reg_train_r2s, '-ob', label = regression_type)\n",
        "    axes[0].set(title='Training data', ylabel='R2', xlabel='alpha', xticks = np.arange(0, len(alphas)), xticklabels = alphas)\n",
        "\n",
        "    axes[0].legend()\n",
        "\n",
        "    axes[1].plot([0, len(alphas)], [validation_r2, validation_r2], 'g')\n",
        "    axes[1].plot(reg_validation_r2s, '-ob')\n",
        "    axes[1].set(title='Validation data', ylabel='R2', xlabel='alpha', xticks = np.arange(0, len(alphas)), xticklabels= alphas);"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TCUlgD2L2y7",
        "cellView": "form"
      },
      "source": [
        "#@markdown Data retrieval\n",
        "import os, requests\n",
        "\n",
        "fname = \"stringer_spontaneous.npy\"\n",
        "url = \"https://osf.io/dpqaj/download\"\n",
        "\n",
        "if not os.path.isfile(fname):\n",
        "  try:\n",
        "    r = requests.get(url)\n",
        "  except requests.ConnectionError:\n",
        "    print(\"!!! Failed to download data !!!\")\n",
        "  else:\n",
        "    if r.status_code != requests.codes.ok:\n",
        "      print(\"!!! Failed to download data !!!\")\n",
        "    else:\n",
        "      with open(fname, \"wb\") as fid:\n",
        "        fid.write(r.content)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NzmDSn6Mskux"
      },
      "source": [
        "Run the next two cells to download the data and fit all the models while you read as it takes a bit of time! I will explain later what we're doing in this cell."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sHgxFmXms07h",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute to load data\n",
        "# Load data\n",
        "dat = np.load('stringer_spontaneous.npy', allow_pickle=True).item()\n",
        "\n",
        "# Split into train and validation and test\n",
        "# (not a great way to split, don't do as I do here)\n",
        "resps_train = dat['sresp'].T[5000:7000]\n",
        "run_train = dat['run'][5000:7000]\n",
        "\n",
        "resps_validation = dat['sresp'].T[2000:2250]\n",
        "run_validation = dat['run'][2000:2250, :]\n",
        "\n",
        "\n",
        "resps_test = dat['sresp'].T[2250:2500]\n",
        "run_test = dat['run'][2250:2500, :]\n",
        "\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FiH0ClgTBeuA"
      },
      "source": [
        "# The data\n",
        "\n",
        "We are going to use real neural data! We'll decode the running speed of a mouse from simultaneously recorded neurons. Specifically, we'll look at the activity of ~10,000 neurons while the mouse is behaving spontaneously in [this study](https://science.sciencemag.org/content/364/6437/eaav7893) (provided by Carsen Stringer and NMA). Our task will be to decode the running speed from the responses of the whole population of neurons. The neural responses were recorded by calcium imaging so we're not working with spikes but with the  fluorescence traces. We will use linear regression and study how regularization affects the weights of the neurons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0dr0odDsd_Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f8abf66-c4e4-42d4-a0aa-5816dd8cdb69"
      },
      "source": [
        "print(resps_train.shape)\n",
        "print(run_train.shape)\n",
        "\n",
        "print(resps_validation.shape)\n",
        "print(run_validation.shape)\n",
        "\n",
        "print(resps_test.shape)\n",
        "print(run_test.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2000, 11983)\n",
            "(2000, 1)\n",
            "(250, 11983)\n",
            "(250, 1)\n",
            "(250, 11983)\n",
            "(250, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g3UJP5ra11es"
      },
      "source": [
        "# Section 1: Fitting linear regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6YtJ0FWFlbzg"
      },
      "source": [
        "First, we'll examine a standard linear regression model and examine our prediction quality on held out data. We fit this model using sklearn.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "\n",
        "no_reg_model = make_pipeline(StandardScaler(with_mean=False), LinearRegression())\n",
        "\n",
        "no_reg_model.fit(resps_train, run_train)\n",
        "\n",
        "y_pred = no_reg_model.predict(resps_train)\n",
        "\n",
        "R2 = no_reg_model.score(resps_train, run_train)"
      ],
      "metadata": {
        "id": "89RmUIjlYOSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 246
        },
        "outputId": "e9ec40c5-3235-40f1-e75f-721f700c5946"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-75e9c709344c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mno_reg_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_pipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mStandardScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_mean\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mno_reg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresps_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mno_reg_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresps_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'resps_train' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's four steps in the code above:\n",
        "\n",
        "- We *initialized* the model.\n",
        "We have a standard scalar as the first step in our pipeline so the model standardizes the input data (the neural activities) before fitting.\n",
        "\n",
        "- We *fit* the model by passing it the training data\n",
        "- We *predicted* y (running speed) given resps_train.\n",
        "- We *evaluated* the model by returning the R2 coefficient of determination of the prediction (this is given by the score method)"
      ],
      "metadata": {
        "id": "LZAekL2CYaHt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 1: Model goodness on held out data.\n",
        "\n",
        "Print the goodness of the model (R2 value) on training data and on testing data.\n",
        "\n",
        "Hint: we've already fit the model on training data for you (above) and our test data is `X_test` and `y_test`"
      ],
      "metadata": {
        "id": "O7VLDPLkZILj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_r2 = ...\n",
        "\n",
        "validation_r2 = ...\n",
        "\n",
        "print(f'The train R2 is {train_r2}')\n",
        "print(f'The validation R2 is {validation_r2}')"
      ],
      "metadata": {
        "id": "e8p-DjtQZm6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's plot the predicted running speed vs true running speed on the training and test data."
      ],
      "metadata": {
        "id": "bTX-A41WZ0LJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OnxL1BMFlbPg"
      },
      "source": [
        "# Compute predictions\n",
        "preds_train = no_reg_model.predict(resps_train)\n",
        "preds_validation = no_reg_model.predict(resps_validation)\n",
        "\n",
        "# Make plot\n",
        "plot_model_quality(run_train, preds_train, run_validation, preds_validation, train_r2, validation_r2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8qAPfSyvnnw"
      },
      "source": [
        "\n",
        "We can see that we are predicting the training data very accurately (high R2/overlapping traces) but are not predicting the validation data as well. Seems like we're overfitting! We can try some regularization to fix this.\n",
        "\n",
        "(We are still fitting the validation data fairly well so we can decode running speed quite well from the neural activity!)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " An even better choice would have been to not have separate validation data. Instaed, we could use cross-validation to fit and evaluate on the training + validation data so we use it all. However, this takes too long for the purposes of this tutorial.\n",
        "\n",
        "Just so you know though, we can do cross-validation easily easily in one line of code using the magic of scikit-learn. See the cell below for evaluating the model using 5 folds cross validation"
      ],
      "metadata": {
        "id": "kJvGIY0HaSwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_validate\n",
        "scores = cross_validate(make_pipeline(StandardScaler(with_mean=False), LinearRegression()), resps_train, run_train,\n",
        "                        cv = 5, return_train_score = True, return_estimator=True)\n",
        "\n",
        "print(f\"Cross-validated training score is {scores['train_score'].mean()}\")\n",
        "print(f\"Cross-validated validation score is {scores['test_score'].mean()}\")"
      ],
      "metadata": {
        "id": "0D_oFCEHaWEX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Setting up weight plots\n",
        "\n",
        "Let's first look at our fit weights using logistic regression without regularization:"
      ],
      "metadata": {
        "id": "3gYGwz1Sawvp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_weights({\"No regularization\": no_reg_model[1]})"
      ],
      "metadata": {
        "id": "pLudOSrgbsw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's important to understand this plot. Each dot visualizes a value in our parameter vector $\\theta$. Since each feature is the time-averaged response of a neuron, each dot shows how the model uses each neuron to estimate running speed."
      ],
      "metadata": {
        "id": "O32cv0Zcb7jW"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p7cQPG6I3noP"
      },
      "source": [
        "# Section 2: Fitting models with  L2 regularization\n",
        "\n",
        "\n",
        "We will first look at models with  L2 (Ridge) regularization.\n",
        "\n",
        "\n",
        "Remember that in this model we add the term $\\alpha \\sum_i \\theta_i^2 $ to the loss function we are minimizing. We want the sum of squared weights to be small.\n",
        "\n",
        "In sklearn, we fit L2 regularized linear regression models as:\n",
        "\n",
        "```\n",
        "model = make_pipeline(StandardScaler(with_mean=False), Ridge(alpha = 1))\n",
        "model.fit(resps_train, run_train)\n",
        "```\n",
        "\n",
        "Again, the `alpha` input tells the weighting of the regularization term in the loss function."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 2: Choosing the regularization amount\n",
        "\n",
        "How do you know what value of alpha to use?\n",
        "\n",
        "The answer is the same as when you want to know whether you have learned good parameter values: use cross-validation. The best hyperparameter will be the one that allows the model to generalize best to unseen data. Let's use L2 regularization and pick a good penalty. Fill out the following code to do this.\n",
        "\n",
        "Instead of doing K-folds cross-validation, we will fit the model on the training data and use the validation data for cross-validation. We have totally held out test data we can use to report model performance later."
      ],
      "metadata": {
        "id": "g-jRjL1esIcp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ridge/L2 regularization\n",
        "l2_alphas = [1, 10, 100, 1000, 10000, 100000]\n",
        "\n",
        "ridge_models = {}\n",
        "ridge_train_r2s = np.zeros((len(l2_alphas)))\n",
        "ridge_validation_r2s = np.zeros(len(l2_alphas))\n",
        "\n",
        "# Your code here to loop over different values of alpha, fit a Ridge regression model on the training data,\n",
        "  # store the resulting R2 score on the training and validation data in the correct entry of `ridge_train_r2s` and\n",
        "  # `ridge_validation_r2s`\n",
        "  # Save the fit model for a given alpha in the dictionary `ridge_models` - the key should be the value of alpha\n",
        "\n",
        "\n",
        "# Visualize\n",
        "plot_regularized_train_and_val(l2_alphas, ridge_train_r2s, ridge_validation_r2s, train_r2, validation_r2, 'Ridge')"
      ],
      "metadata": {
        "id": "x85nVySRsqj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Zlx82yv6xQJ"
      },
      "source": [
        "We can see that by increasing regularization, we decrease performance on training data. Here, we increase performance (for some values of regularization) on validation data more. For high values of regularization, our model does very badly on both train and validation data - we have to pick the regularization amount carefully!"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 3: Effect of L2 regularization on weights\n",
        "\n",
        "Let's compare the unregularized classifier weights with the classifier weights with different values of alpha (the strength of the regularization).\n",
        "\n",
        "**What effect does high L2 regularization (high alpha) seem to have on the weights? Please be specific.**"
      ],
      "metadata": {
        "id": "66X4bAW6ryjE"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4ZXjcclytJS",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute cell to enable widget\n",
        "\n",
        "@widgets.interact\n",
        "def plot_observed(alpha = l2_alphas):\n",
        "  models = {\n",
        "    \"No regularization\": no_reg_model[1],\n",
        "    \"Regularization\": ridge_models[alpha][1]\n",
        "  }\n",
        "  plot_weights(models)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your text answer here"
      ],
      "metadata": {
        "id": "hHIxmqycr63w"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "desXc8Bo3Q3e"
      },
      "source": [
        "# Section 3: Fitting models with L1 Regularization\n",
        "\n",
        "We will first look at models with  L1 (Lasso) regularization.\n",
        "\n",
        "Remember that in this model we add the term $\\alpha \\sum_i | \\beta_i | $ to the loss function we are minimizing. We want the sum of the absolute values of the weights to be small.\n",
        "\n",
        "In sklearn, we fit L1 regularized linear regression models as:\n",
        "\n",
        "```\n",
        "model = make_pipeline(StandardScaler(with_mean=False), Lasso(alpha))\n",
        "model.fit(resps_train, run_train)\n",
        "```\n",
        "\n",
        "The `alpha` input tells the weighting of the regularization term in the loss function. With higher values of alpha, there will be more regularization. Technically, alpha = 0 is linear regression without regularization but there is numerical instability if you do this option in sklearn (so just use LinearRegression instead).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 4: Choosing the regularization amount\n",
        "\n",
        "As in exercise 2, let's loop over regularization amounts and fit the model. This time though, we'll fit the Lasso model."
      ],
      "metadata": {
        "id": "ygv8nU0Y8_yw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Lasso/L1 regularization\n",
        "l1_alphas = [0.01, 0.1, 1, 10]\n",
        "\n",
        "lasso_models = {}\n",
        "lasso_train_r2s = np.zeros((len(l1_alphas)))\n",
        "lasso_validation_r2s = np.zeros(len(l1_alphas))\n",
        "\n",
        "# Your code here to loop over different values of alpha, fit a Lasso regression model on the training data,\n",
        "  # store the resulting R2 score on the training and validation data in the correct entry of `lasso_train_r2s` and\n",
        "  # `lasso_validation_r2s`\n",
        "  # Save the fit model for a given alpha in the dictionary `lasso_models` - the key should be the value of alpha\n",
        "\n",
        "\n",
        "# Visualize\n",
        "plot_regularized_train_and_val(l1_alphas, lasso_train_r2s, lasso_validation_r2s, train_r2, validation_r2, 'Lasso')"
      ],
      "metadata": {
        "id": "KRITSbPm9KQ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujEewQWG4kel"
      },
      "source": [
        "We can see that with increasing values of alpha, the performance on the training data decreases. On validation data, we get slightly better performance for certain values of alpha (0.001) as compared to the model with no regularization. For high values of regularization, our model does very badly on both train and validation data - we have to pick the regularization amount carefully!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0H4Dzc6-hWB"
      },
      "source": [
        "## Exercise 5: Effect of L1 regularization on weights\n",
        "\n",
        "Let's compare the unregularized classifier weights with the classifier weights with different values of alpha (the strength of the regularization).\n",
        "\n",
        "**What effect does high L1 regularization (high alpha) seem to have on the weights? Please be specific.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtqYlVck9z7o",
        "cellView": "form"
      },
      "source": [
        "# @markdown Execute cell to enable widget\n",
        "@widgets.interact\n",
        "def plot_observed(alpha = l1_alphas):\n",
        "  models = {\n",
        "    \"No regularization\": no_reg_model[1],\n",
        "    \"Regularization\": lasso_models[alpha][1]\n",
        "  }\n",
        "  plot_weights(models)\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "8RXJRbUQ9hUD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Section 4: Choosing regularizers\n",
        "\n",
        "\n",
        "When should you use $L_1$ vs. $L_2$ regularization? Both penalties shrink parameters, and both will help reduce overfitting. However, the models they lead to are different.\n",
        "\n",
        "In particular, the $L_1$ penalty encourages *sparse* solutions in which most parameters are 0. Let's unpack the notion of sparsity.\n",
        "\n",
        "A \"dense\" vector has mostly nonzero elements:\n",
        "$\\begin{bmatrix}\n",
        "  0.1 \\\\ -0.6\\\\-9.1\\\\0.07\n",
        "\\end{bmatrix}$.\n",
        "A \"sparse\" vector has mostly zero elements:\n",
        "$\\begin{bmatrix}\n",
        "  0 \\\\ -0.7\\\\ 0\\\\0\n",
        "\\end{bmatrix}$.\n",
        "\n",
        "The same is true of matrices:"
      ],
      "metadata": {
        "id": "0l39V0L19j0-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @markdown Execute to plot a dense and a sparse matrix\n",
        "np.random.seed(50)\n",
        "n = 5\n",
        "M = np.random.random((n, n))\n",
        "M_sparse = np.random.choice([0,1], size=(n, n), p=[0.8, 0.2])\n",
        "\n",
        "fig, axs = plt.subplots(1, 2, sharey=True, figsize=(10,5))\n",
        "\n",
        "axs[0].imshow(M)\n",
        "axs[1].imshow(M_sparse)\n",
        "axs[0].axis('off')\n",
        "axs[1].axis('off')\n",
        "axs[0].set_title(\"A dense matrix\", fontsize=15)\n",
        "axs[1].set_title(\"A sparse matrix\", fontsize=15)\n",
        "text_kws = dict(ha=\"center\", va=\"center\")\n",
        "for i in range(n):\n",
        "  for j in range(n):\n",
        "    iter_parts = axs, [M, M_sparse], [\"{:.1f}\", \"{:d}\"]\n",
        "    for ax, mat, fmt in zip(*iter_parts):\n",
        "      val = mat[i, j]\n",
        "      color = \".1\" if val > .7 else \"w\"\n",
        "      ax.text(j, i, fmt.format(val), c=color, **text_kws)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "4ftdEiFK9gRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Larger L1 regularization leads to sparser solutions.\n",
        "\n",
        "When is it OK to assume that the parameter vector is sparse? Whenever it is true that most features don't affect the outcome. One use-case might be decoding low-level visual features from whole-brain fMRI: we may expect only voxels in V1 and thalamus should be used in the prediction.\n",
        "\n",
        "**WARNING**: be careful when interpreting $\\theta$. Never interpret the nonzero coefficients as *evidence* that only those voxels/neurons/features carry information about the outcome. This is a product of our regularization scheme, and thus *our prior assumption that the solution is sparse*. Other regularization types or models may find very distributed relationships across the brain. Never use a model as evidence for a phenomena when that phenomena is encoded in the assumptions of the model.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "sXsUjI339sNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exercise 6: Final evaluation on test data\n"
      ],
      "metadata": {
        "id": "74H2MGXq-HMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A) You can also use your cross-validation scores to choose the type of regularization! Based on `ridge_validation_r2s` and `lasso_validation_r2s` - what model would you select? Which type of regularization (if any) and what would the value of alpha be?\n",
        "\n"
      ],
      "metadata": {
        "id": "w4GZhgzg9169"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "YOUR ANSWER HERE"
      ],
      "metadata": {
        "id": "zPjpSv6l-dbl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "B) Now we want to report our model of choice's performance on the completely held-out test data. Note we didn't use this test data for any model selection!\n",
        "\n",
        "**Grab the correct model based on your answer to A and report the R2 score of the predictions on the test data.**\n",
        "\n",
        "Remember that you stored your models in the `ridge_models` and `lasso_models` dictionary where the key is the alpha value. So you should be able to retrieve the Rdige model with alpha 1 as `ridge_models[1]` for example"
      ],
      "metadata": {
        "id": "ldsVp6Cm-q41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# your code here"
      ],
      "metadata": {
        "id": "KmfsTXuG-Ib4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we could have done an even more comprehensive search and used both ridge and lasso regularization, changing the weights of each! This would be a *grid search* over hyperparameters (as we'd try every combo of Lasso alpha and Ridge alpha in a grid-like pattern). We won't do this for time but sklearn's `ElasticNet` (https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html) allows you to fit a linear regression model with both types of regularization."
      ],
      "metadata": {
        "id": "eLIAznLz-cr3"
      }
    }
  ]
}