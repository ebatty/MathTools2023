{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyOV2uCLW9vHyfVZIWpCbNbg",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "view-in-github",
    "colab_type": "text"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/ebatty/MathToolsforNeuroscience/blob/jupyterbook/Week10/Week10Tutorial1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9l0lyqLvkW7"
   },
   "source": [
    "# Tutorial 1\n",
    "\n",
    "**Machine Learning II: Model Selection**\n",
    "\n",
    "**[insert your name]**\n",
    "\n",
    "**Important reminders**: Before starting, click \"File -> Save a copy in Drive\". Produce a pdf for submission by \"File -> Print\" and then choose \"Save to PDF\".\n",
    "\n",
    "To complete this tutorial, you should have watched Video 10.1, 10.2, and 10.3\n",
    "\n",
    "**This tutorial is inspired by and uses text/code from NMA W1D3, which in turn was inspired by Eero Simoncelli's Math Tools course**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cv9HSBNPyLV9",
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# @markdown Imports\n",
    "\n",
    "# Imports\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets  # interactive display\n",
    "import math"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Plotting functions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZIdPVYl9TzmK",
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# @markdown Plotting functions\n",
    "import numpy\n",
    "from numpy.linalg import inv, eig\n",
    "from math import ceil\n",
    "from matplotlib import pyplot, ticker, get_backend, rc\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from itertools import cycle\n",
    "\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.style.use(\"https://raw.githubusercontent.com/NeuromatchAcademy/course-content/master/nma.mplstyle\")\n",
    "\n",
    "def plot_fitted_polynomials(x, y, theta_hat):\n",
    "  \"\"\" Plot polynomials of different orders\n",
    "\n",
    "  Args:\n",
    "    x (ndarray): input vector of shape (n_samples)\n",
    "    y (ndarray): vector of measurements of shape (n_samples)\n",
    "    theta_hat (dict): polynomial regression weights for different orders\n",
    "  \"\"\"\n",
    "\n",
    "  x_grid = np.linspace(x.min() - .5, x.max() + .5)\n",
    "\n",
    "  plt.figure()\n",
    "\n",
    "  for order in range(0, max_order + 1):\n",
    "    X_design = make_design_matrix(x_grid, order)\n",
    "    plt.plot(x_grid, X_design @ theta_hat[order]);\n",
    "\n",
    "  plt.ylabel('y')\n",
    "  plt.xlabel('x')\n",
    "  plt.plot(x, y, 'C0.');\n",
    "  plt.legend([f'order {o}' for o in range(max_order + 1)], loc=1)\n",
    "  plt.title('polynomial fits')\n",
    "  plt.show()\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Helper functions\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "0TCUlgD2L2y7",
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# @markdown Helper functions\n",
    "\n",
    "def ordinary_least_squares(X, y):\n",
    "  \"\"\"Ordinary least squares estimator for linear regression.\n",
    "\n",
    "  Args:\n",
    "    x (ndarray): design matrix of shape (n_samples, n_regressors)\n",
    "    y (ndarray): vector of measurements of shape (n_samples)\n",
    "\n",
    "  Returns:\n",
    "    ndarray: estimated parameter values of shape (n_regressors)\n",
    "  \"\"\"\n",
    "  \n",
    "  # Compute theta_hat using OLS\n",
    "  theta_hat = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "  return theta_hat\n",
    "\n",
    "\n",
    "def evaluate_poly_reg(x, y, theta_hat, order):\n",
    "    \"\"\" Evaluates MSE of polynomial regression models on data\n",
    "\n",
    "    Args:\n",
    "      x (ndarray): input vector of shape (n_samples)\n",
    "      y (ndarray): vector of measurements of shape (n_samples)\n",
    "      theta_hats (dict):  fitted weights for each polynomial model (dict key is order)\n",
    "      max_order (scalar): max order of polynomial fit\n",
    "\n",
    "    Returns\n",
    "      (ndarray): mean squared error for each order, shape (max_order)\n",
    "    \"\"\"\n",
    "\n",
    "    X_design = make_design_matrix(x, order)\n",
    "\n",
    "    y_hat = np.dot(X_design, theta_hats[order])\n",
    "\n",
    "    residuals = y - y_hat\n",
    "\n",
    "    mse = np.mean(residuals ** 2)\n",
    "\n",
    "    return mse\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiH0ClgTBeuA"
   },
   "source": [
    "# Exercise 1: Polynomial regression\n",
    "\n",
    "We can extend linear regression to capture more complex nonlinear relationships by using polynomial regression.  Linear regression models predict the outputs as a weighted sum of the inputs:\n",
    "\n",
    "$$y_{n}= \\theta_0 + \\theta x_{n} + \\epsilon_{n}$$\n",
    "\n",
    "With polynomial regression, we model the outputs as a polynomial equation based on the inputs. For example, we can model the outputs as:\n",
    "\n",
    "$$y_{n}= \\theta_0 + \\theta_1 x_{n} + \\theta_2 x_{n}^2 + \\theta_3 x_{n}^3 + \\epsilon_{n}$$\n",
    "\n",
    "We can change how complex a polynomial is fit by changing the order of the polynomial. The order of a polynomial refers to the highest power in the polynomial. The equation above is a third order polynomial because the highest value x is raised to is 3. We could add another term ($+ \\theta_4 x_{n}^4$) to model an order 4 polynomial and so on.\n",
    "\n",
    "Execute the next cell to generate and plot the data we will fit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute this cell to simulate some data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ENd8JqlJBgvZ",
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "#@markdown Execute this cell to simulate some data\n",
    "\n",
    "### Generate training data\n",
    "np.random.seed(0)\n",
    "n_train_samples = 50\n",
    "x_train = np.random.uniform(-2, 2.5, n_train_samples) # sample from a uniform distribution over [-2, 2.5)\n",
    "noise = np.random.randn(n_train_samples) # sample from a standard normal distribution\n",
    "y_train =  x_train**2 - x_train - 2 + noise\n",
    "\n",
    "### Generate testing data\n",
    "n_test_samples = 20\n",
    "x_test = np.random.uniform(-3, 3, n_test_samples) # sample from a uniform distribution over [-2, 2.5)\n",
    "noise = np.random.randn(n_test_samples) # sample from a standard normal distribution\n",
    "y_test =  x_test**2 - x_test - 2 + noise\n",
    "\n",
    "## Plot both train and test data\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Data')\n",
    "plt.plot(x_train, y_train, '.', markersize=15, label='Training')\n",
    "#plt.plot(x_test, y_test, 'g+', markersize=15, label='Test')\n",
    "#plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8QIAPLswCg3v"
   },
   "source": [
    "## A) Structuring the design matrix\n",
    "\n",
    "\n",
    "For linear regression, we used $X = x$ as our design matrix. To add a constant bias (a y-intercept in a 2-D plot), we use $X = \\big[ \\boldsymbol 1, x \\big]$, where $\\boldsymbol 1$ is a column of ones.  When fitting, we learn a weight for each column of this matrix. So we learn a weight that multiples with column 1 - in this case that column is all ones so we gain the bias parameter ($+ \\theta_0$). We also learn a weight for every column, or every feature of x. \n",
    "\n",
    "The key difference between fitting a linear regression model and a polynomial regression model lies in how we create the design matrix. How should we construct the design matrix $X$ so that a 3rd order polynomial regression model can be in matrix form as $Y = X\\theta$? What is $\\theta$?  Write out the matrix multiplication for a row of $Y$ to show it works out equal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMzj5Eg1FvLQ"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na-WmeHEEzog"
   },
   "source": [
    "## B) Coding the design matrix\n",
    "\n",
    "Complete the function below(`make_design_matrix`) to structure the design matrix given the input data and the order of the polynomial you wish to fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cs13JVBfF8Xw"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Complete code below"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OvWuFMZMCxuk"
   },
   "source": [
    "def make_design_matrix(x, order):\n",
    "  \"\"\"Create the design matrix of inputs for use in polynomial regression\n",
    "\n",
    "  Args:\n",
    "    x (ndarray): input vector of shape (n_samples)\n",
    "    order (scalar): polynomial regression order\n",
    "\n",
    "  Returns:\n",
    "    ndarray: design matrix for polynomial regression of shape (samples, order+1)\n",
    "  \"\"\"\n",
    "\n",
    "  # Broadcast to shape (n x 1) so dimensions work\n",
    "  if x.ndim == 1:\n",
    "    x = x[:, None]\n",
    "\n",
    "  #if x has more than one feature, we don't want multiple columns of ones so we assign\n",
    "  # x^0 here\n",
    "  design_matrix = np.ones((x.shape[0], 1))\n",
    "\n",
    "  # Finish creating the design matrix (hint: np.hstack)\n",
    "  for degree in range(1, order + 1):\n",
    "      design_matrix = ...\n",
    "\n",
    "  return design_matrix\n",
    "\n",
    "\n",
    "order = 5\n",
    "X_design = make_design_matrix(x_train, order)\n",
    "print(X_design[0:2, 0:2])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w-qNAF6fDoxt"
   },
   "source": [
    "## Fitting polynomial regression models\n",
    "\n",
    "We will use this design matrix function to fit polynomial regression models of different orders. We are doing this using the same function we completed in Week 9 Tutorial 1 (`ordinary_least_squares`). I provide you with the code below but just make sure you understand what is happening."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mbSFAV6CDiio"
   },
   "source": [
    "def solve_poly_reg(x, y, order):\n",
    "  \"\"\"Fit a polynomial regression model for a given order.\n",
    "\n",
    "  Args:\n",
    "    x (ndarray): input vector of shape (n_samples)\n",
    "    y (ndarray): vector of measurements of shape (n_samples)\n",
    "    order (scalar): order of polynomial fit\n",
    "\n",
    "  Returns:\n",
    "    ndarray: fitted weights of polynomial model\n",
    "  \"\"\"\n",
    "\n",
    "  # Create design matrix\n",
    "  X_design = make_design_matrix(x, order)\n",
    "\n",
    "  # Fit polynomial model (use ordinary_least_squares)\n",
    "  theta_hat = ordinary_least_squares(X_design, y)\n",
    "\n",
    "  return theta_hat\n",
    "\n",
    "\n",
    "# Loop over several orders and fit polynomial regressions\n",
    "max_order = 5\n",
    "theta_hats = {}\n",
    "for order in range(max_order + 1):\n",
    "  theta_hats[order] = solve_poly_reg(x_train, y_train, order)\n",
    "\n",
    "plot_fitted_polynomials(x_train, y_train, theta_hats)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FIw8iiCHQb_A"
   },
   "source": [
    "**In this exercise, we saw we could create a polynomial regression model by using the linear regression setup and altering the design matrix. Altering the design matrix and using linear regression is a powerful tool! For example, in neuroscience, we might want to fit temporal lags (so recent history of a stimulus, not just the current frame). We can create the design matrix in such a way as to include the temporal lags**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEEpYL8_BhUQ"
   },
   "source": [
    "# Exercise 2: Bias variance tradeoff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hz_8qeW8Hliy"
   },
   "source": [
    "## A) Thinking about models\n",
    "\n",
    "**Answer these before moving on**\n",
    "\n",
    "These questions relate to the models we've just fit in Exercise 1:\n",
    "\n",
    "i) Which model do you think will have the lowest training MSE (mean squared error on the data with which the models are fit)? Why? \n",
    "\n",
    "ii) Which model do you think could have lowest test MSE? Why? (we'll accept several answers as long as the reasoning is good since it's hard to tell from the plot)\n",
    "\n",
    "iii) Which model has lowest variance (and highest bias)? Why?\n",
    "\n",
    "iv) Which model has high variance and low bias? Why?\n",
    "\n",
    "**Now look at the next sections to validate your answers.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dTA9EjOCGtNr"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hnmg_XqXHrug"
   },
   "source": [
    "## Looking at bias vs variance\n",
    "\n",
    "In the plots below, I have resampled new data sets (new samples) from the true data distribution. I have fit each polynomial order to each new sample. Note that we cannot usually resample data from the actual distribution - we would normally use bootstrapping! \n",
    "\n",
    "I am plotting the true data model in black and each fit model in green."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute to visualize multiple fits\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "8tOIILrsIl0T",
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# @markdown Execute to visualize multiple fits\n",
    "np.random.seed(121)\n",
    "\n",
    "fig, axes = plt.subplots(1, 6, figsize = (15, 4), sharey=True)\n",
    "\n",
    "\n",
    "x_grid = np.linspace(x_train.min() - .5, x_train.max() + .5)\n",
    "X_design = {}\n",
    "y_hats = {}\n",
    "for order in range(max_order + 1):\n",
    "  X_design[order] = make_design_matrix(x_grid, order)\n",
    "  y_hats[order] = np.zeros((0, len(x_grid)))\n",
    "\n",
    "for i_sample in range(30):\n",
    "\n",
    "  # Sample new data\n",
    "  n_samples = 30\n",
    "  x = np.random.uniform(-2, 2.5, n_samples)  # inputs uniformly sampled from [-2, 2.5)\n",
    "  y =  x**2 - x - 2   # computing the outputs\n",
    "\n",
    "  output_noise = 1.5 * np.random.randn(n_samples)\n",
    "  y += output_noise  # adding some output noise\n",
    "\n",
    "  # Loop over several orders and fit polynomial regressions\n",
    "  max_order = 5\n",
    "  theta_hats = {}\n",
    "  for order in range(max_order + 1):\n",
    "    theta_hats[order] = solve_poly_reg(x, y, order)\n",
    "\n",
    "    y_hat = X_design[order] @ theta_hats[order]\n",
    "    y_hats[order] = np.concatenate((y_hats[order], y_hat[None, :]), axis=0)\n",
    "    axes[order].plot(x_grid, y_hat, 'g', alpha=.2, label='Fitted models' if i_sample == 0 else \"\")\n",
    "\n",
    "for order in range(max_order + 1):\n",
    "  axes[order].plot(x_grid, X_design[2] @ np.array([-2, -1, 1]), 'k', label='True model')\n",
    "  axes[order].set(ylim=[-15, 15], xlabel='x', ylabel='y', title='Order '+str(order))\n",
    "\n",
    "leg = axes[0].legend(loc='best', frameon=False, handlelength=0)\n",
    "# change the font colors to match the line colors:\n",
    "for line,text in zip(leg.get_lines(), leg.get_texts()):\n",
    "    text.set_color(line.get_color())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yuf5nhiyG9My"
   },
   "source": [
    "\n",
    "We can see that the order 5 model fits vary a lot based on the sample of data used - there is a lot of **variance** over different data samples. On average though, these fits resemble the true model (if you average over the green lines, it is roughly the black line) so this model has low **bias**. The order 0 model has high bias since the predictions do not match the true model on average (but lower variance than the order 5 model). By eye, the order 2 model looks about right as a balance between these tendencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0BKHsmR1HugX"
   },
   "source": [
    "## Looking at train vs test MSE\n",
    "\n",
    "We can compute the mean squared error of each polynomial model on the data it is fit with (the training data) and held-out data (the test data). **No need to answer this explicitly here but discuss how the training MSE should change over order and how the test MSE should before seeing the plot.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute to see train and test data\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iyq4nxDuO7S9",
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# @markdown Execute to see train and test data\n",
    "## Plot both train and test data\n",
    "fig, ax = plt.subplots()\n",
    "plt.title('Training & Test Data')\n",
    "plt.plot(x_train, y_train, '.', markersize=15, label='Training')\n",
    "plt.plot(x_test, y_test, 'g+', markersize=15, label='Test')\n",
    "plt.legend()\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Execute to see train vs test MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rDj_YSYsBiuf",
    "cellView": "form",
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# @markdown Execute to see train vs test MSE\n",
    "mse_train = np.zeros((max_order+1))\n",
    "mse_test = np.zeros((max_order+1))\n",
    "for order in range(max_order + 1):\n",
    "  theta_hat = solve_poly_reg(x_train, y_train, order)\n",
    "  mse_train[order] = evaluate_poly_reg(x_train, y_train, theta_hat, order)\n",
    "  mse_test[order] = evaluate_poly_reg(x_test, y_test, theta_hat, order)\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "width = .35\n",
    "\n",
    "ax.bar(np.arange(max_order + 1) - width / 2, mse_train, width, label=\"train MSE\")\n",
    "ax.bar(np.arange(max_order + 1) + width / 2, mse_test , width, label=\"test MSE\")\n",
    "\n",
    "ax.legend()\n",
    "ax.set(xlabel='Polynomial order', ylabel='MSE', title ='Comparing polynomial fits');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_KOxK4VHBjkl"
   },
   "source": [
    "# Exercise 3: Model selection via cross validation\n",
    "\n",
    "In Exercise 2, we sampled multiple times from the actual distribution and looked at the test data to think about best model fits - neither of which we can usually do! Let's do model selection properly using cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n6oJkciiE5Jm"
   },
   "source": [
    "## A) By hand implementation on a toy example\n",
    "\n",
    "We usually use sklearn for cross-validation in python. We will first implement cross-validation for a tiny toy example to make sure we understand the steps.\n",
    "\n",
    "Let's say we have:\n",
    "\n",
    "$$ x =   \\begin{bmatrix}\n",
    "    5 & 10 & 2 & 3 & 1 & 6\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "  $$ y = \\begin{bmatrix}\n",
    "   4 & 3 & 1 & 6 & 3 & 4\n",
    "  \\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "We want to fit the model: $$y = \\theta$$ The least squares solution (see derivation below) is $$\\hat{\\theta} = \\frac{\\sum y_i}{N}$$\n",
    "\n",
    "Note that this is a standard baseline model - we are essentially modeling y as the average of y in the training data. This would be like predicting the responses of a neuron by using the mean firing rate.\n",
    "\n",
    "Use 3 fold cross validation (by hand) and report the validation MSE for this model on this data. You may use code or do this by hand, but don't use any functions for cross validation. Show your work (i.e. specify how you're splitting the data, report the validation MSE for every split, etc). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l2ca2LCnObDR"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y0snSXooOPzo"
   },
   "source": [
    "We could do the same procedure for $y = \\theta x$ and use the validation MSE to pick which model is better (telling us whether y is at all correlated with x). We won't though for time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyNRTS5sM78m"
   },
   "source": [
    "*Derivation of least squares solution*\n",
    "$$\\begin{align}\n",
    "y &= \\theta \\\\\n",
    "MSE & = \\frac{1}{N} \\sum_i (y_i - \\hat{y}_i)^2 \\\\\n",
    "& = \\frac{1}{N} \\sum_i (y_i - \\theta)^2 \\\\\n",
    "\\frac{dMSE}{d\\theta} &= \\frac{2}{N} \\sum_i (y_i - \\theta) \\\\\n",
    "&= \\sum_i (y_i - \\theta)  \\\\\n",
    "&= \\sum_i y_i - \\sum_i \\theta  \\\\\n",
    "&= \\sum_i y_i - N\\theta = 0 \\\\\n",
    "\\hat{\\theta} &= \\frac{\\sum_i y_i}{N}\n",
    "\\end{align}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jb3OCb7yFCBt"
   },
   "source": [
    "## B) Using sklearn \n",
    "\n",
    "Now we can use sklearn to perform cross-validation and select the our model from the different order polynomial models. Here I just use sklearn to get the train/val splits (using the `Kfold` iterator) and then use our functions above to fit the models. At the end of this tutorial, I show how to do everything (including fitting the models) in sklearn.\n",
    "\n",
    "Nothing to do here but look at the plot and answer the question - I show you the code in case you want to look through it."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "gU4q4wx5O-zi"
   },
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def cross_validate(x_train, y_train, max_order, n_splits):\n",
    "  \"\"\" Compute MSE for k-fold validation for each order polynomial\n",
    "\n",
    "  Args:\n",
    "    x_train (ndarray): training data input vector of shape (n_samples)\n",
    "    y_train (ndarray): training vector of measurements of shape (n_samples)\n",
    "    max_order (scalar): max order of polynomial fit\n",
    "    n_split (scalar): number of folds for k-fold validation\n",
    "\n",
    "  Return:\n",
    "    ndarray: MSE over splits for each model order, shape (n_splits, max_order + 1)\n",
    "\n",
    "  \"\"\"\n",
    "  # Initialize the split method\n",
    "  kfold_iterator = KFold(n_splits)\n",
    "\n",
    "  # Initialize np array mse values for all models for each split\n",
    "  mse_all = np.zeros((n_splits, max_order + 1))\n",
    "\n",
    "  for i_split, (train_indices, val_indices) in enumerate(kfold_iterator.split(x_train)):\n",
    "\n",
    "      # Split up the overall training data into cross-validation training and validation sets\n",
    "      x_cv_train = x_train[train_indices]\n",
    "      y_cv_train = y_train[train_indices]\n",
    "      x_cv_val = x_train[val_indices]\n",
    "      y_cv_val = y_train[val_indices]\n",
    "\n",
    "      # Fit and evaluate models\n",
    "      for order in range(max_order + 1):\n",
    "          theta_hat = solve_poly_reg(x_cv_train, y_cv_train, order)\n",
    "          mse_all[i_split, order] = evaluate_poly_reg(x_cv_val, y_cv_val, theta_hat, order)\n",
    "\n",
    "  return mse_all\n",
    "\n",
    "\n",
    "# Call function and plot\n",
    "max_order = 5\n",
    "n_splits = 10\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "mse_all = cross_validate(x_train, y_train, max_order, n_splits)\n",
    "plt.boxplot(mse_all, labels=np.arange(0, max_order + 1))\n",
    "\n",
    "plt.xlabel('Polynomial Order')\n",
    "plt.ylabel('Validation MSE')\n",
    "plt.title(f'Validation MSE over {n_splits} splits of the data');"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KhK0qDUcWEc2"
   },
   "source": [
    "Which polynomial order do you think is a better model of the data based on cross-validation? Why?\n",
    "\n",
    "Note it may not be what you expected - we'll discuss in class!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RRAdN_a-WIFD"
   },
   "source": [
    "### Answer\n",
    "\n",
    "<font color='green'><span style=\"font-size:larger;\">\n",
    "Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cufR7pGUSB2t"
   },
   "source": [
    "# Coding Fun: Everything in sklearn\n",
    "\n",
    "Let's look at implementing polynomial regression models, fitting them, and performing cross-validation entirely in sklearn.\n",
    "\n",
    "We create a polynomial regression model by chaining together a preprocessing step that turns the data into polynomial features and a linear regression model. (Note that if we have more than 1D data, this would create interaction terms which we may not want).\n",
    "\n",
    "See here for more details: https://towardsdatascience.com/polynomial-regression-with-scikit-learn-what-you-should-know-bed9d3296f2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_JeoxsrNb8Pf"
   },
   "source": [
    "The code below fits an order 3 polynomial regression model, looks at the fitted parameters, and gets predictions/R2 value"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4A-sZm6KSCoH"
   },
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Create polynomial regression model\n",
    "degree = 3\n",
    "poly_reg = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "\n",
    "# Fit this model to data\n",
    "poly_reg.fit(x_train[:, None], y_train)\n",
    "\n",
    "# You can now look at theta_hat\n",
    "print(poly_reg['linearregression'].coef_)\n",
    "\n",
    "# Let's make predictions on test data\n",
    "y_hat = poly_reg.predict(x_test[:, None])\n",
    "\n",
    "# Let's evaluate on test data (this returns R2)\n",
    "poly_reg.score(x_test[:, None], y_test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFFKK9PtcDGA"
   },
   "source": [
    "The code below does cross validation with 10 splits for an order 3 polynomial regression model. We could loop over this to get the validation MSE for each order model."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xbXEy3umZKA6"
   },
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "## Let's get validation MSE for this model using cross-validation\n",
    "\n",
    "# Create polynomial regression model\n",
    "degree = 3\n",
    "poly_reg = make_pipeline(PolynomialFeatures(degree),LinearRegression())\n",
    "\n",
    "# Cross-validation\n",
    "val_mse = cross_val_score(poly_reg, x_train[:, None], y_train, cv = 10, scoring='neg_mean_squared_error')\n",
    "\n",
    "# This has return negative MSE so let's get actual MSE\n",
    "val_mse *= -1\n",
    "\n",
    "print(val_mse)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZS4_GF1cTuD"
   },
   "source": [
    "Let's get really fancy and using `GridSearchCV` to search over all the orders of polynomial regression we're looking at and perform cross-validation for each."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "HBwqLoYYbQ_n"
   },
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Parameters we want to search over\n",
    "parameters = {'polynomialfeatures__degree': [0, 1, 2, 3, 4, 5]}\n",
    "\n",
    "# Set up cross validation\n",
    "poly_reg = make_pipeline(PolynomialFeatures(),LinearRegression())\n",
    "\n",
    "cv = GridSearchCV(poly_reg, parameters, cv = 10, scoring='neg_mean_squared_error')\n",
    "cv.fit(x_train[:, None], y_train)\n",
    "\n",
    "plt.plot(np.arange(max_order + 1), -1*cv.cv_results_['mean_test_score'], '-og')\n",
    "plt.xlabel('Order')\n",
    "plt.ylabel('Validation MSE');"
   ],
   "execution_count": null,
   "outputs": []
  }
 ]
}